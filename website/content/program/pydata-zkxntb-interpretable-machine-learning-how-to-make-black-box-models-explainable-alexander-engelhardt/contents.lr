_model: session 
---
code: ZKXNTB
---
title: Interpretable Machine Learning: How to make black box models explainable
---
description: In this talk, we'll find out how to interpret the predictions of otherwise black-box models.
---
short_description: In this talk, we'll find out how to interpret the predictions of otherwise black-box models.
---
twitter_image: /static/media/twitter/ZKXNTB.jpg
---
speakers: Alexander Engelhardt
---
submission_type: Talk
---
domains: Data Science, Machine Learning
---
biography: #### Alexander Engelhardt

Affiliation: Engelhardt Data Science GmbH



Statistician turned freelance data scientist, Munich based.

Caught the entrepreneurial bug. Now experimenting with product-based business and/or productized services.

visit the speaker at: [Twitter](https://twitter.com/eng_elhardt) • [Github](https://github.com/alexengelhardt) • [Homepage](http://www.engelhardt-ds.de)
---
affiliation: Engelhardt Data Science GmbH
---
track: PyData
---
python_skill: Python Skill Level basic
---
domain_expertise: Domain Expertise some
---
room: Saal 2
---
start_time: 14:00
---
day: friday
---
meta_title: Interpretable Machine Learning: How to make black box models explainable Alexander Engelhardt PyConDE & PyDataBerlin 2019 conference 
---
meta_twitter_title: Interpretable Machine Learning: How to make black box models explainable @eng_elhardt #PyConDE #PyDataBerlin #PyData
---
categories: pydata, python-skill-level-basic, domain-expertise-some, talk, data-science, machine-learning, friday, friday-1400
---
slugified_slot_links: friday, friday-1400
---
video_link: https://www.youtube.com/embed/sAqSGY-HkVY
---
youtube_id: sAqSGY-HkVY
---
body: We all love linear regression for its interpretability: Increase square
meters by 1, that leads to the rent going up by 8 euros. A human can easily
understand why this model made a certain prediction.

Complex machine learning models like tree aggregates or neural networks
usually make better predictions, but this comes at a price: it's hard to
understand these models.

In this talk, we'll look at a few common problems of black-box models, e.g. unwanted discrimination or unexplainable false predictions ("bugs"). Next, we go over three methods to pry open these models and gain some insights into how and why they make their predictions.

I'll conclude with a few predictions about the future of (interpretable) machine learning.
   
Specifically, the topics covered are

- What makes a model interpretable?
  - Linear models, trees
- How to understand your model
- Model-agnostic methods for interpretability
  - Permutation Feature Importance
  - Partial dependence plots (PDPs)
  - Shapley Values / SHAP
- The future of (interpretable) machine learning

