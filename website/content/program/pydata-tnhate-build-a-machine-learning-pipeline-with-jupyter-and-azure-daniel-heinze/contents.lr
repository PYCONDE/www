_model: session 
---
code: TNHATE
---
title: Build a Machine Learning pipeline with Jupyter and Azure
---
description: Build a Machine Learning pipeline with Jupyter and Azure: https://notebooks.azure.com/Starlord/projects/pycon-ml-jupyter-azure
---
short_description: Build a Machine Learning pipeline with Jupyter and Azure: https://notebooks.azure.com/Starlord/projects/pycon-ml-jupyter-azure
---
twitter_image: /static/media/twitter/TNHATE.jpg
---
speakers: Daniel Heinze
---
submission_type: Tutorial
---
domains: Computer Vision, Deep Learning, DevOps, IDEs/ Jupyter, Machine Learning, APIs, Python
---
biography: #### Daniel Heinze

Affiliation: Microsoft



Daniel is a Data Engineer at Microsoft, working with customers to create services that get insights from data and take that to improve the system through Machine Learning.

visit the speaker at: [Github](https://github.com/starlord-daniel)
---
affiliation: Microsoft
---
track: PyData
---
python_skill: Python Skill Level basic
---
domain_expertise: Domain Expertise some
---
room: Saal 4
---
start_time: 11:30
---
day: thursday
---
meta_title: Build a Machine Learning pipeline with Jupyter and Azure Daniel Heinze PyConDE & PyDataBerlin 2019 conference 
---
meta_twitter_title: Build a Machine Learning pipeline with Jupyter and Azure @ #PyConDE #PyDataBerlin #PyData
---
categories: pydata, python-skill-level-basic, domain-expertise-some, tutorial, computer-vision, deep-learning, devops, ides-jupyter, machine-learning, apis, python, thursday, thursday-1130
---
slugified_slot_links: thursday, thursday-1130
---
video_link: 
---
youtube_id: 
---
body: With increasing focus on Machine Learning systems in almost every business area, it is important to build a great pipeline to train, test and deploy your models. In this session we will show a way to do that with Jupyter and Azure.
The session will cover the following topics:
- creating a simple image classification service without coding
- creating a PyTorch model from scratch
- Training and Testing the PyTorch model
- Saving the model locally and on a cloud storage (with self-made versioning)
- evaluating multiple models
- deploying an API (via Docker) to get predictions from the model
- using DevOps to update the API

Notes:

Please bring your own device, as we will be running the workshop on individual machines.

To prepare:
- make sure you have a Python environment >= 3.5 installed (preferred using Anaconda)
- we will be using Azure, so make sure you have a Microsoft account - no Azure registration needed beforehand
- optional: Install Docker on your machine, install VS Code, install Azure Storage Explorer

